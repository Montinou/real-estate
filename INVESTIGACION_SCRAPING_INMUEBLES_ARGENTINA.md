# Investigaci√≥n Completa: Scraping de Inmuebles en Argentina (C√≥rdoba)

**Fecha:** 2025-11-10
**Objetivo:** Motor de procesamiento automatizado para scraping de propiedades inmobiliarias en Argentina, con foco en C√≥rdoba
**Estado:** Investigaci√≥n Completa - Ready for Implementation

---

## üìã Tabla de Contenidos

1. [Resumen Ejecutivo](#resumen-ejecutivo)
2. [Fuentes de Datos Disponibles](#fuentes-de-datos-disponibles)
3. [Arquitectura de Base de Datos](#arquitectura-de-base-de-datos)
4. [Almacenamiento de Im√°genes](#almacenamiento-de-im√°genes)
5. [Motor de Procesamiento Automatizado](#motor-de-procesamiento-automatizado)
6. [Estrategia de Implementaci√≥n](#estrategia-de-implementaci√≥n)
7. [Consideraciones Legales](#consideraciones-legales)
8. [Costos y Presupuesto](#costos-y-presupuesto)
9. [Roadmap de Desarrollo](#roadmap-de-desarrollo)
10. [Referencias y Recursos](#referencias-y-recursos)

---

## üéØ Resumen Ejecutivo

### Objetivo del Proyecto
Crear un motor automatizado de scraping, procesamiento y almacenamiento de datos de inmuebles de m√∫ltiples fuentes en Argentina, especialmente enfocado en la provincia de C√≥rdoba, con capacidad de:

- Scrapear m√∫ltiples portales inmobiliarios
- Normalizar y deduplicar datos de diferentes fuentes
- Almacenar y servir im√°genes optimizadas
- Trackear historial de precios y cambios
- Detectar propiedades vendidas/inactivas
- Proveer API para b√∫squedas avanzadas

### Hallazgos Clave

1. **Acceso Legal Disponible**: MercadoLibre y Properati ofrecen acceso oficial a datos
2. **Volumen Significativo**: ~65,000+ propiedades disponibles solo en C√≥rdoba
3. **Stack Recomendado**: Supabase + Crawlee + BullMQ + ScraperAPI
4. **Inversi√≥n Inicial MVP**: ~$335/mes para 50k propiedades
5. **Tiempo de Implementaci√≥n**: 12-16 semanas para MVP completo

### Recomendaci√≥n Principal

**Comenzar con fuentes legales (MercadoLibre API + Properati BigQuery)** para validar el modelo de negocio, y luego expandir progresivamente a scraping controlado de otras fuentes con protocolos √©ticos y rate limiting respetuoso.

---

## üìä Fuentes de Datos Disponibles

### 1. MercadoLibre Inmuebles ‚úÖ **RECOMENDADO**

**API Oficial:** S√ç - OAuth 2.0

#### Caracter√≠sticas
- **Cobertura:** 400,000+ listings en Argentina
- **Documentaci√≥n:** https://developers.mercadolibre.com.ar/
- **Autenticaci√≥n:** OAuth 2.0 est√°ndar
- **Rate Limits:** No especificados p√∫blicamente (consultar docs)
- **Costo:** GRATIS (solo necesitas registrar tu app)

#### Endpoints Principales
```bash
# B√∫squeda de inmuebles
GET /sites/MLA/search?category=MLA1459

# Detalle de propiedad
GET /items/{ITEM_ID}

# B√∫squeda geolocalizada
GET /sites/MLA/search?item_location=lat:$LAT1_LAT2,lon:$LON1_LON2&category=MLA1459
```

#### Datos Disponibles
- ID √∫nico (formato MLA + n√∫meros)
- T√≠tulo y descripci√≥n completa
- Precio y moneda (ARS/USD)
- Ubicaci√≥n con coordenadas (lat/lon)
- Tipo de inmueble y operaci√≥n
- Atributos estructurados: ambientes, ba√±os, m¬≤
- Im√°genes m√∫ltiples
- Informaci√≥n del vendedor con reputaci√≥n

#### Ejemplo de Respuesta
```json
{
  "id": "MLA2544295250",
  "title": "Departamento 2 ambientes Nueva C√≥rdoba",
  "price": 85000,
  "currency_id": "USD",
  "latitude": -31.4201,
  "longitude": -64.1888,
  "attributes": [
    {
      "id": "MLA1459-INMUEBLE",
      "name": "Inmueble",
      "value_name": "Departamento"
    },
    {
      "id": "MLA1466-AMBQTY",
      "name": "Ambientes",
      "value_name": "2"
    },
    {
      "id": "MLA1466-MTRS",
      "name": "Superficie cubierta (m¬≤)",
      "value_name": "45"
    }
  ],
  "pictures": [...],
  "seller": {
    "id": 123456,
    "nickname": "INMOBILIARIA_XYZ"
  }
}
```

#### Ventajas
- ‚úÖ 100% legal y con soporte oficial
- ‚úÖ Datos estructurados de alta calidad
- ‚úÖ No requiere scraping ni proxies
- ‚úÖ OAuth 2.0 est√°ndar de industria
- ‚úÖ Documentaci√≥n completa en espa√±ol

---

### 2. Properati ‚úÖ **RECOMENDADO**

**Dataset P√∫blico:** S√ç - Google BigQuery

#### Caracter√≠sticas
- **Cobertura:** 2,000,000+ propiedades (Argentina, Brasil, M√©xico, Chile, Colombia, Per√∫)
- **C√≥rdoba:** 10,572 inmuebles
- **Acceso:** Google BigQuery (base de datos p√∫blica)
- **Prop√≥sito:** Acad√©micos, periodistas, investigadores
- **Costo:** **GRATIS**

#### Acceso
1. Ir a Google Cloud Marketplace
2. Buscar "Properati Property Data Argentina"
3. Conectar a tu proyecto de BigQuery
4. Consultas SQL est√°ndar

#### Schema del Dataset

```sql
-- Campos principales
SELECT
    id,                              -- ID √∫nico
    created_on,                      -- Fecha de publicaci√≥n
    property_type,                   -- "Apartment", "House", "PH", etc.
    operation_type,                  -- "sale", "rent"
    lat, lon,                        -- Coordenadas
    place_with_parent_names,         -- "Argentina|C√≥rdoba|C√≥rdoba|Nueva C√≥rdoba"
    price,                           -- Precio original
    price_aprox_usd,                 -- Precio en USD
    currency,
    price_usd_per_m2,
    rooms, bedrooms,                 -- Ambientes y dormitorios
    surface_total_in_m2,
    surface_covered_in_m2,
    floor,
    expenses,                        -- Expensas
    title, description,
    properati_url
FROM `properati-data-public.properties_ar.properties_ar`
WHERE place_with_parent_names LIKE '%C√≥rdoba%'
```

#### Ejemplo de Query
```sql
-- Departamentos en venta en Nueva C√≥rdoba
SELECT
    id,
    title,
    price_aprox_usd,
    rooms,
    surface_total_in_m2,
    properati_url
FROM `properati-data-public.properties_ar.properties_ar`
WHERE
    place_with_parent_names LIKE '%Nueva C√≥rdoba%'
    AND property_type = 'Apartment'
    AND operation_type = 'sale'
    AND price_aprox_usd BETWEEN 50000 AND 150000
ORDER BY price_aprox_usd
LIMIT 100
```

#### Ventajas
- ‚úÖ 100% legal y oficial
- ‚úÖ Datos hist√≥ricos masivos
- ‚úÖ Gratis para uso no comercial
- ‚úÖ Schema bien estructurado
- ‚úÖ M√∫ltiples pa√≠ses LATAM
- ‚úÖ Ideal para an√°lisis de mercado

---

### 3. ZonaProp ‚ö†Ô∏è **RIESGO MEDIO**

**API Oficial:** NO
**Scrapeabilidad:** Medio-Alto

#### Caracter√≠sticas
- **Cobertura C√≥rdoba:** 54,648 propiedades (la m√°s grande)
- **Protecciones:** JavaScript rendering, 403 Forbidden para bots
- **Rate Limiting:** Probable (no confirmado)
- **Robots.txt:** Restrictivo (bloquea paginaci√≥n >5, ciertos patrones)

#### Datos Disponibles (v√≠a scraping)
- 33+ campos extra√≠bles
- URL, t√≠tulo, descripci√≥n
- Precio (ARS/USD)
- Ubicaci√≥n completa + coordenadas
- Tipo de propiedad y operaci√≥n
- Ambientes, ba√±os, superficie (total y cubierta)
- Im√°genes m√∫ltiples
- Datos de inmobiliaria
- Expensas, fecha de publicaci√≥n, n√∫mero de visitas

#### Protecciones T√©cnicas
- **403 Forbidden** para user-agents no est√°ndar
- **JavaScript rendering** obligatorio (SPA)
- **reCAPTCHA** en registro/login
- Bloqueo de IPs reportado por desarrolladores

#### Robots.txt - Restricciones
```
Disallow: /develop/
Disallow: /mails/
Disallow: /tracking/g/*
Disallow: *?duplicated=true
Disallow: *?labs=
Disallow: /*-ubicado-en-* (excepto /propiedades/)
Disallow: p√°ginas 6+ en paginaci√≥n

Allow: /propiedades/*-ubicado-en-*
Allow: p√°ginas 2-5
Allow: *-orden-precio-ascendente.html
```

#### T√©rminos de Servicio
- ‚ö†Ô∏è No permiso expl√≠cito para scraping
- ‚ö†Ô∏è Pueden bloquear IPs
- ‚ö†Ô∏è No se permite extracci√≥n masiva de datos

#### Estrategia de Scraping (si se implementa)
```javascript
// Rate limiting OBLIGATORIO
const config = {
  maxRequestsPerSecond: 0.5,        // 1 request cada 2 segundos
  randomDelay: [2000, 5000],        // Delays aleatorios 2-5 seg
  respectRobotsTxt: true,           // Respetar robots.txt
  userAgent: 'RealEstateAggregator/1.0 (+https://mysite.com; contact@email.com)',
  maxConcurrency: 1,                // Sin paralelizaci√≥n
  retryDelay: 60000,                // 1 minuto en retry
  headless: 'new',                  // Modo headless moderno
  stealthPlugin: true               // Anti-detecci√≥n
}
```

#### Scrapers Existentes (GitHub)
- **mauroeparis/scrappdept** - Multi-portal scraper (Python)
- **rodrigouroz/housing_scrapper** - Incluye ZonaProp (Python)
- **Sotrosca/zona-prop-scraper** - Espec√≠fico ZonaProp (Python)

---

### 4. Argenprop ‚ùå **ALTO RIESGO - NO RECOMENDADO**

**API Oficial:** NO
**Scrapeabilidad:** Medio-Alto con alto riesgo legal

#### Caracter√≠sticas
- **Cobertura:** 430,000+ propiedades en Argentina
- **Protecciones:** reCAPTCHA, bloqueo agresivo de bots
- **Robots.txt:** MUY restrictivo (70+ bots bloqueados)

#### ‚ö†Ô∏è ADVERTENCIA LEGAL - MUY IMPORTANTE
**Argenprop PROH√çBE EXPL√çCITAMENTE el scraping en sus t√©rminos:**

- ‚ùå **Prohibido extraer contenido** (scraping) del sitio o base de datos
- ‚ùå **Prohibido obtener listas de inventario** o informaci√≥n privada
- ‚ùå **Pueden bloquear IPs** por scraping (confirmado)
- ‚ùå **Violaci√≥n de derechos de autor** - materiales protegidos
- ‚öñÔ∏è **Jurisdicci√≥n:** Tribunales Comerciales Nacionales, Capital Federal, Argentina
- ‚öñÔ∏è **Ley aplicable:** Leyes de la Rep√∫blica Argentina

#### Recomendaci√≥n
**NO IMPLEMENTAR** scraping de Argenprop para uso comercial. Alto riesgo legal y de bloqueo permanente.

---

### 5. Inmuebles24 ‚ö†Ô∏è **RIESGO MEDIO**

**API Oficial:** NO
**Propietario:** Grupo Navent (mismo que ZonaProp) / QuintoAndar

#### Caracter√≠sticas
- Tecnolog√≠a y backend compartidos con ZonaProp
- Robots.txt id√©ntico a ZonaProp
- 403 Forbidden para bots
- APIs privadas protegidas

#### Recomendaci√≥n
Tratamiento similar a ZonaProp. Si se implementa scraping, usar mismas precauciones.

---

### 6. La Voz Clasificados ‚ö†Ô∏è **MUY DIF√çCIL**

**API Oficial:** NO
**Scrapeabilidad:** Muy dif√≠cil

#### Caracter√≠sticas
- **Enfoque:** Regional C√≥rdoba
- **Protecciones:** Muy agresivas, bloqueo total a herramientas autom√°ticas
- **Status:** No pudimos acceder en la investigaci√≥n (Claude Code bloqueado)

#### Recomendaci√≥n
Dejar para fase avanzada, requiere investigaci√≥n adicional y t√©cnicas especializadas.

---

### 7. Otras Fuentes

#### CordobaProp
- Portal regional espec√≠fico de C√≥rdoba
- Profesionales registrados en CPI
- Sin API p√∫blica conocida
- Menor volumen pero 100% enfocado en C√≥rdoba

#### RE/MAX
- Red de franquicias inmobiliarias
- 80,338 propiedades en C√≥rdoba
- Sin API p√∫blica
- Listings duplicados en otros portales

---

## üèóÔ∏è Arquitectura de Base de Datos

### Elecci√≥n de Base de Datos

**Recomendaci√≥n: PostgreSQL 15+ con Supabase**

#### Justificaci√≥n

**PostgreSQL:**
- ‚úÖ Datos estructurados y relacionales
- ‚úÖ **PostGIS** para b√∫squedas geoespaciales
- ‚úÖ **JSONB** para flexibilidad por fuente
- ‚úÖ **pg_trgm** para fuzzy matching (deduplicaci√≥n)
- ‚úÖ Triggers y funciones para automatizaci√≥n
- ‚úÖ TimescaleDB para series temporales (historial de precios)
- ‚úÖ Madurez y ecosistema robusto

**Supabase:**
- ‚úÖ Consistencia con tu stack actual
- ‚úÖ PostgreSQL completo con extensiones
- ‚úÖ Row-Level Security (RLS)
- ‚úÖ Real-time subscriptions
- ‚úÖ API REST/GraphQL autom√°tica
- ‚úÖ Studio visual para administraci√≥n
- ‚úÖ Backups autom√°ticos diarios

**Descartando NoSQL:**
- Las propiedades tienen estructura com√∫n suficiente
- Relaciones importantes (fuentes, historial, deduplicaci√≥n)
- PostgreSQL con JSONB ofrece flexibilidad h√≠brida

---

### Modelo de Datos Completo

#### Diagrama de Alto Nivel

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ     SOURCES     ‚îÇ ‚Üê Fuentes de scraping
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
         ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   RAW_LISTINGS  ‚îÇ ‚Üê Datos crudos sin procesar
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
         ‚Üì (Processing & Deduplication)
         ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   PROPERTIES    ‚îÇ ‚Üê Propiedades normalizadas (MASTER)
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
         ‚îú‚Üí PROPERTY_HISTORY     ‚Üê Historial de cambios
         ‚îú‚Üí PRICE_HISTORY        ‚Üê Track de precios
         ‚îú‚Üí PROPERTY_DUPLICATES  ‚Üê Clusters de duplicados
         ‚îú‚Üí PROPERTY_IMAGES      ‚Üê Im√°genes
         ‚îî‚Üí PROPERTY_SOURCES     ‚Üê Relaci√≥n M2M con fuentes
```

#### Tablas Principales

##### 1. SOURCES
```sql
CREATE TABLE sources (
    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    name VARCHAR(100) NOT NULL UNIQUE,           -- 'zonaprop', 'mercadolibre'
    display_name VARCHAR(200) NOT NULL,          -- 'ZonaProp', 'MercadoLibre'
    base_url TEXT NOT NULL,
    scraper_config JSONB,                        -- Config del scraper
    is_active BOOLEAN DEFAULT true,
    reliability_score DECIMAL(3,2) DEFAULT 0.80, -- 0-1
    last_scrape_at TIMESTAMPTZ,
    created_at TIMESTAMPTZ DEFAULT NOW(),
    updated_at TIMESTAMPTZ DEFAULT NOW()
);
```

##### 2. RAW_LISTINGS (Datos Crudos)
```sql
CREATE TABLE raw_listings (
    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    source_id UUID NOT NULL REFERENCES sources(id),
    external_id VARCHAR(255) NOT NULL,           -- ID en la fuente
    url TEXT NOT NULL,

    raw_data JSONB NOT NULL,                     -- JSON completo

    -- Campos extra√≠dos para b√∫squeda r√°pida
    title TEXT,
    price_raw VARCHAR(100),
    location_raw TEXT,

    -- Metadata
    scraped_at TIMESTAMPTZ DEFAULT NOW(),
    processing_status listing_status DEFAULT 'pending',
    processed_at TIMESTAMPTZ,
    error_message TEXT,
    property_id UUID REFERENCES properties(id),

    created_at TIMESTAMPTZ DEFAULT NOW(),
    UNIQUE(source_id, external_id)
);

CREATE INDEX idx_raw_listings_status ON raw_listings(processing_status);
CREATE INDEX idx_raw_listings_data ON raw_listings USING gin(raw_data);
```

##### 3. PROPERTIES (Master - Normalizado)
```sql
CREATE TABLE properties (
    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),

    -- Identificaci√≥n
    internal_code VARCHAR(50) UNIQUE,

    -- Tipo
    property_type property_type NOT NULL,        -- ENUM
    operation_type operation_type NOT NULL,      -- ENUM
    status property_status DEFAULT 'active',     -- ENUM

    -- Ubicaci√≥n
    country VARCHAR(2) DEFAULT 'AR',
    province VARCHAR(100),                       -- C√≥rdoba, Buenos Aires, etc.
    city VARCHAR(200),
    neighborhood VARCHAR(200),
    address TEXT,
    street_name VARCHAR(200),
    street_number VARCHAR(20),
    floor VARCHAR(20),
    apartment VARCHAR(20),
    postal_code VARCHAR(20),

    -- Geolocalizaci√≥n (PostGIS)
    location GEOMETRY(Point, 4326),              -- WGS84
    location_confidence DECIMAL(3,2),            -- 0-1

    -- Precio
    price DECIMAL(15,2),
    currency currency_type DEFAULT 'ARS',        -- ENUM
    price_usd DECIMAL(15,2),
    expenses DECIMAL(12,2),
    expenses_currency currency_type DEFAULT 'ARS',

    -- Caracter√≠sticas
    total_surface DECIMAL(10,2),
    covered_surface DECIMAL(10,2),
    rooms INTEGER,
    bedrooms INTEGER,
    bathrooms INTEGER,
    garage_spaces INTEGER,
    age_years INTEGER,

    -- Descripci√≥n
    title TEXT,
    description TEXT,

    -- Features adicionales (JSONB flexible)
    features JSONB,                              -- {amenities: [...], services: [...]}

    -- Metadata
    first_seen_at TIMESTAMPTZ DEFAULT NOW(),
    last_seen_at TIMESTAMPTZ DEFAULT NOW(),
    last_price_change_at TIMESTAMPTZ,
    status_changed_at TIMESTAMPTZ,
    times_seen INTEGER DEFAULT 1,

    -- Calidad
    data_quality_score DECIMAL(3,2) DEFAULT 0.50,
    is_verified BOOLEAN DEFAULT false,

    -- Deduplicaci√≥n
    duplicate_cluster_id UUID,
    is_canonical BOOLEAN DEFAULT true,

    created_at TIMESTAMPTZ DEFAULT NOW(),
    updated_at TIMESTAMPTZ DEFAULT NOW()
);

-- √çndices cr√≠ticos
CREATE INDEX idx_properties_type ON properties(property_type);
CREATE INDEX idx_properties_operation ON properties(operation_type);
CREATE INDEX idx_properties_location ON properties USING GIST(location);
CREATE INDEX idx_properties_province_city ON properties(province, city);
CREATE INDEX idx_properties_price_usd ON properties(price_usd);
CREATE INDEX idx_properties_title_trgm ON properties USING gin(title gin_trgm_ops);
CREATE INDEX idx_properties_features ON properties USING gin(features);
```

##### 4. PROPERTY_HISTORY (Historial de Cambios)
```sql
CREATE TABLE property_history (
    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    property_id UUID NOT NULL REFERENCES properties(id) ON DELETE CASCADE,

    snapshot JSONB NOT NULL,                     -- Snapshot completo
    changed_fields TEXT[],                       -- ['price', 'status']
    changes_summary JSONB,                       -- {field: {old, new}}

    recorded_at TIMESTAMPTZ DEFAULT NOW(),
    source_id UUID REFERENCES sources(id),
    created_at TIMESTAMPTZ DEFAULT NOW()
);

CREATE INDEX idx_property_history_property ON property_history(property_id);
CREATE INDEX idx_property_history_recorded ON property_history(recorded_at DESC);
```

##### 5. PRICE_HISTORY (Series Temporales con TimescaleDB)
```sql
CREATE TABLE price_history (
    property_id UUID NOT NULL REFERENCES properties(id) ON DELETE CASCADE,
    price DECIMAL(15,2) NOT NULL,
    currency currency_type NOT NULL,
    price_usd DECIMAL(15,2),
    exchange_rate DECIMAL(12,4),
    recorded_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),
    source_id UUID REFERENCES sources(id),

    PRIMARY KEY (property_id, recorded_at)
);

-- Convertir a hypertable de TimescaleDB
SELECT create_hypertable('price_history', 'recorded_at',
    chunk_time_interval => INTERVAL '1 month',
    if_not_exists => TRUE
);

CREATE INDEX idx_price_history_property ON price_history(property_id, recorded_at DESC);
```

##### 6. PROPERTY_DUPLICATES (Deduplicaci√≥n)
```sql
CREATE TABLE property_duplicates (
    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    cluster_id UUID NOT NULL,
    property_id UUID NOT NULL REFERENCES properties(id) ON DELETE CASCADE,
    canonical_property_id UUID REFERENCES properties(id),

    similarity_score DECIMAL(5,4),               -- 0-1
    match_method VARCHAR(50),                    -- 'coordinates', 'address', 'fuzzy'
    match_details JSONB,

    detected_at TIMESTAMPTZ DEFAULT NOW(),
    verified_by UUID,
    is_confirmed BOOLEAN DEFAULT false,
    created_at TIMESTAMPTZ DEFAULT NOW(),

    UNIQUE(property_id)
);

CREATE INDEX idx_property_duplicates_cluster ON property_duplicates(cluster_id);
CREATE INDEX idx_property_duplicates_property ON property_duplicates(property_id);
```

##### 7. PROPERTY_IMAGES
```sql
CREATE TABLE property_images (
    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    property_id UUID NOT NULL REFERENCES properties(id) ON DELETE CASCADE,

    url TEXT NOT NULL,
    thumbnail_url TEXT,

    display_order INTEGER DEFAULT 0,
    width INTEGER,
    height INTEGER,
    file_size INTEGER,
    image_hash VARCHAR(64),                      -- MD5/SHA256 para dedupe

    created_at TIMESTAMPTZ DEFAULT NOW()
);

CREATE INDEX idx_property_images_property ON property_images(property_id, display_order);
CREATE INDEX idx_property_images_hash ON property_images(image_hash);
```

##### 8. PROPERTY_SOURCES (Relaci√≥n M2M)
```sql
CREATE TABLE property_sources (
    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    property_id UUID NOT NULL REFERENCES properties(id) ON DELETE CASCADE,
    source_id UUID NOT NULL REFERENCES sources(id) ON DELETE CASCADE,
    external_id VARCHAR(255) NOT NULL,
    external_url TEXT,

    first_seen_at TIMESTAMPTZ DEFAULT NOW(),
    last_seen_at TIMESTAMPTZ DEFAULT NOW(),
    times_seen INTEGER DEFAULT 1,
    is_active BOOLEAN DEFAULT true,

    created_at TIMESTAMPTZ DEFAULT NOW(),
    updated_at TIMESTAMPTZ DEFAULT NOW(),
    UNIQUE(property_id, source_id)
);

CREATE INDEX idx_property_sources_property ON property_sources(property_id);
CREATE INDEX idx_property_sources_source ON property_sources(source_id);
```

---

### Estrategia de Deduplicaci√≥n

#### Algoritmo Multi-Etapa

```sql
CREATE OR REPLACE FUNCTION deduplicate_property(p_property_id UUID)
RETURNS TABLE(duplicate_id UUID, similarity_score DECIMAL, match_method VARCHAR)
AS $$
DECLARE
    prop RECORD;
    potential_dup RECORD;
    score DECIMAL;
BEGIN
    SELECT * INTO prop FROM properties WHERE id = p_property_id;

    -- ETAPA 1: Match por coordenadas (radio 50m)
    FOR potential_dup IN
        SELECT
            p2.id,
            ST_Distance(prop.location::geography, p2.location::geography) as distance
        FROM properties p2
        WHERE p2.id != prop.id
          AND ST_DWithin(prop.location::geography, p2.location::geography, 50)
    LOOP
        score := 1.0 - (potential_dup.distance / 50.0);
        RETURN QUERY SELECT potential_dup.id, score, 'coordinates'::VARCHAR;
    END LOOP;

    -- ETAPA 2: Match por direcci√≥n + caracter√≠sticas
    FOR potential_dup IN
        SELECT
            p2.id,
            similarity(
                LOWER(CONCAT(prop.street_name, prop.street_number, prop.apartment)),
                LOWER(CONCAT(p2.street_name, p2.street_number, p2.apartment))
            ) as address_sim,
            CASE
                WHEN prop.total_surface IS NOT NULL AND p2.total_surface IS NOT NULL THEN
                    1.0 - ABS(prop.total_surface - p2.total_surface) / GREATEST(prop.total_surface, p2.total_surface)
                ELSE 0
            END as surface_sim
        FROM properties p2
        WHERE p2.id != prop.id
          AND p2.city = prop.city
          AND p2.neighborhood = prop.neighborhood
          AND p2.property_type = prop.property_type
    LOOP
        score := (potential_dup.address_sim * 0.6 + potential_dup.surface_sim * 0.4);

        IF score >= 0.75 THEN
            RETURN QUERY SELECT potential_dup.id, score, 'address_features'::VARCHAR;
        END IF;
    END LOOP;

    -- ETAPA 3: Fuzzy matching de t√≠tulo
    FOR potential_dup IN
        SELECT
            p2.id,
            similarity(LOWER(prop.title), LOWER(p2.title)) as title_sim
        FROM properties p2
        WHERE p2.id != prop.id
          AND p2.city = prop.city
          AND similarity(LOWER(prop.title), LOWER(p2.title)) > 0.8
    LOOP
        RETURN QUERY SELECT potential_dup.id, potential_dup.title_sim, 'fuzzy_title'::VARCHAR;
    END LOOP;
END;
$$ LANGUAGE plpgsql;
```

#### Campos Clave para Matching

1. **Coordenadas** (m√°xima prioridad)
   - Radio de 50 metros con PostGIS
   - Score: 1.0 - (distancia/50)

2. **Direcci√≥n normalizada** (alta prioridad)
   - Calle + n√∫mero + piso + depto
   - Fuzzy matching con pg_trgm
   - Peso: 60%

3. **Caracter√≠sticas f√≠sicas** (media prioridad)
   - Superficie (tolerancia ¬±5%)
   - Peso: 40%

4. **T√≠tulo** (baja prioridad)
   - Fuzzy matching > 0.8
   - Para casos edge

---

### Queries Optimizadas

#### B√∫squeda Geoespacial
```sql
-- Propiedades en venta, radio 2km de coordenada
SELECT
    p.*,
    ST_Distance(
        p.location::geography,
        ST_SetSRID(ST_MakePoint(-64.1888, -31.4201), 4326)::geography
    ) / 1000 AS distance_km
FROM properties p
WHERE
    p.operation_type = 'sale'
    AND p.property_type = 'apartment'
    AND p.status = 'active'
    AND ST_DWithin(
        p.location::geography,
        ST_SetSRID(ST_MakePoint(-64.1888, -31.4201), 4326)::geography,
        2000
    )
ORDER BY distance_km
LIMIT 50;
```

#### B√∫squeda Fuzzy
```sql
-- Match con typos
SELECT
    p.*,
    similarity(p.neighborhood, 'nueva cordoba') as sim_score
FROM properties p
WHERE
    p.city = 'C√≥rdoba'
    AND p.neighborhood % 'nueva cordoba'
    AND p.status = 'active'
ORDER BY sim_score DESC
LIMIT 50;
```

#### Historial de Precios con Variaci√≥n
```sql
-- Cambios de precio √∫ltimos 30 d√≠as
SELECT
    p.id,
    p.address,
    ph.price,
    ph.recorded_at,
    LAG(ph.price) OVER (PARTITION BY p.id ORDER BY ph.recorded_at) as previous_price,
    ROUND(
        ((ph.price - LAG(ph.price) OVER (PARTITION BY p.id ORDER BY ph.recorded_at))
        / LAG(ph.price) OVER (PARTITION BY p.id ORDER BY ph.recorded_at) * 100)::numeric,
        2
    ) as price_change_pct
FROM properties p
JOIN price_history ph ON p.id = ph.property_id
WHERE ph.recorded_at > NOW() - INTERVAL '30 days'
ORDER BY p.id, ph.recorded_at DESC;
```

---

### Estimaci√≥n de Almacenamiento

| Tabla | Registros | Tama√±o/Reg | Total |
|-------|-----------|------------|-------|
| properties | 500K | 5 KB | 2.5 GB |
| raw_listings | 10M | 10 KB | 100 GB |
| property_history | 50M | 3 KB | 150 GB |
| price_history | 100M | 200 B | 20 GB |
| property_images | 5M | 500 B | 2.5 GB |
| **TOTAL** | | | **~275 GB** |

**Supabase Pricing:**
- Free Tier: 500 MB (insuficiente)
- Pro: $25/mes (8GB incluido) + $0.125/GB adicional
- **Estimado:** ~$60/mes para dataset completo

---

## üñºÔ∏è Almacenamiento de Im√°genes

### Recomendaci√≥n: Supabase Storage

#### Por Qu√© Supabase Storage

**Ventajas:**
- ‚úÖ Integrado con tu stack actual (Supabase)
- ‚úÖ CDN global incluido sin costo adicional
- ‚úÖ API simple (REST + cliente JavaScript)
- ‚úÖ RLS (Row-Level Security) integrado
- ‚úÖ Pol√≠ticas de acceso granulares
- ‚úÖ Pricing competitivo
- ‚úÖ Setup en horas, no d√≠as

**Pricing:**
```
Supabase Pro: $25/mes incluye:
- 100 GB storage
- 200 GB bandwidth/mes

Adicional:
- $0.021/GB storage
- $0.09/GB bandwidth

Ejemplo c√°lculo:
- 50,000 im√°genes √ó 200 KB promedio = 10 GB
- Tr√°fico: 100,000 vistas/mes √ó 200 KB = 20 GB
Total: $25/mes (dentro del plan)
```

#### Alternativas Evaluadas

| Soluci√≥n | Mejor Para | Costo (50k im√°genes) |
|----------|-----------|---------------------|
| **Supabase** | Tu caso de uso | $25/mes |
| AWS S3 + CloudFront | Control total | $15-30/mes |
| Cloudinary | Transformaciones avanzadas | $89+/mes |
| Vercel Blob | Apps Next.js | $20-40/mes |
| MinIO Self-hosted | Control absoluto | Variable |

---

### Estrategia de Descarga

**‚ùå NO HACER HOTLINKING**

Razones:
- Ilegal/anti√©tico (robas bandwidth)
- No confiable (pueden eliminar)
- Mal performance
- Problemas de copyright

**‚úÖ DESCARGAR Y ALMACENAR LOCALMENTE**

Razones:
- Control total
- Optimizaci√≥n propia (WebP, AVIF, resize)
- CDN propio
- Performance predecible
- Disponibilidad garantizada

---

### Procesamiento de Im√°genes

#### Pre-generar M√∫ltiples Tama√±os

```javascript
const imageSizes = [
  { name: 'thumbnail', width: 150, quality: 80 },
  { name: 'card', width: 400, quality: 85 },
  { name: 'gallery', width: 800, quality: 90 },
  { name: 'full', width: 1200, quality: 90 }
]

// Generar con Sharp
const sharp = require('sharp')

for (const size of imageSizes) {
  await sharp(originalBuffer)
    .resize(size.width, null, {
      fit: 'inside',
      withoutEnlargement: true
    })
    .webp({ quality: size.quality })
    .toFile(`${size.name}.webp`)
}
```

#### Formatos Modernos

**WebP** (recomendado primario):
- 25-35% m√°s peque√±o que JPEG
- Soporte: 97% navegadores
- Fallback a JPEG autom√°tico

**AVIF** (para 2025+):
- 50% m√°s peque√±o que JPEG
- Soporte: 85% navegadores (creciendo)
- Mejor compresi√≥n que WebP

#### BlurHash para Progressive Loading

```javascript
import { encode } from 'blurhash'

// Generar al procesar imagen
const blurHash = encode(pixels, width, height, 4, 3)
// Ejemplo: "LGF5]+Yk^6#M@-5c,1J5@[or[Q6."

// Guardar en DB
await supabase
  .from('property_images')
  .insert({
    url: imageUrl,
    blur_hash: blurHash
  })

// En frontend (React)
import { Blurhash } from 'react-blurhash'

<Blurhash
  hash={blurHash}
  width={400}
  height={300}
  resolutionX={32}
  resolutionY={32}
  punch={1}
/>
```

---

### Rate Limiting para Descarga

```javascript
const downloadConfig = {
  maxConcurrent: 5,                  // 5 descargas simult√°neas globales
  requestsPerSecond: 1,              // Max 1 req/seg por dominio
  randomDelay: [1000, 3000],         // Delays aleatorios 1-3 seg
  retryAttempts: 3,
  retryDelay: 5000,
  timeout: 30000,

  // Exponential backoff
  backoffMultiplier: 2,
  maxBackoffDelay: 60000
}

// Implementaci√≥n con p-queue
const PQueue = require('p-queue')
const queue = new PQueue({
  concurrency: downloadConfig.maxConcurrent,
  interval: 1000,
  intervalCap: 1
})

async function downloadImage(url, propertyId) {
  return queue.add(async () => {
    const delay = Math.random() *
      (downloadConfig.randomDelay[1] - downloadConfig.randomDelay[0]) +
      downloadConfig.randomDelay[0]

    await new Promise(resolve => setTimeout(resolve, delay))

    const response = await fetch(url, {
      headers: {
        'User-Agent': 'RealEstateAggregator/1.0 (+https://mysite.com; contact@email.com)'
      },
      timeout: downloadConfig.timeout
    })

    if (!response.ok) throw new Error(`HTTP ${response.status}`)

    return response.buffer()
  })
}
```

---

### Organizaci√≥n en Storage

```
supabase-storage/
‚îú‚îÄ‚îÄ properties/
‚îÇ   ‚îú‚îÄ‚îÄ {property_id}/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ original/
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ 001.jpg
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ 002.jpg
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ 003.jpg
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ thumbnail/
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ 001.webp
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ 002.webp
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ 003.webp
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ card/
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ gallery/
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ full/
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ ...
```

#### Pol√≠ticas de Acceso (RLS)

```sql
-- Lectura p√∫blica de im√°genes optimizadas
CREATE POLICY "Public read access for optimized images"
ON storage.objects FOR SELECT
USING (
  bucket_id = 'properties' AND
  (storage.foldername(name))[2] IN ('thumbnail', 'card', 'gallery', 'full')
);

-- Solo admin puede subir originales
CREATE POLICY "Admin upload access"
ON storage.objects FOR INSERT
USING (
  bucket_id = 'properties' AND
  auth.jwt() ->> 'role' = 'admin'
);
```

---

### Aspectos Legales - Im√°genes

**‚ö†Ô∏è Copyright Critical:**

1. **Todas las im√°genes scrapeadas tienen copyright** del portal o inmobiliaria original

2. **Obligatorio documentar source:**
```sql
ALTER TABLE property_images ADD COLUMN source_url TEXT;
ALTER TABLE property_images ADD COLUMN source_attribution TEXT;
ALTER TABLE property_images ADD COLUMN license_type VARCHAR(50);
```

3. **Implementar attribution:**
```html
<!-- En frontend -->
<img src="image.webp" alt="..." />
<small>Fuente: ZonaProp - Inmobiliaria XYZ</small>
```

4. **DMCA Takedown Process:**
   - Email de contacto visible
   - Formulario de reporte
   - Proceso de remoci√≥n en 24-48h
   - Log de takedowns

5. **Mejor approach: Partnerships**
   - Acuerdos con inmobiliarias para uso leg√≠timo
   - Feed oficial de datos e im√°genes
   - Win-win: ellos obtienen exposici√≥n

---

## ‚öôÔ∏è Motor de Procesamiento Automatizado

### Stack Tecnol√≥gico Recomendado

#### Scraping
- **Crawlee** - Framework moderno (successor de Apify SDK)
- **Playwright** - Para sites con JavaScript pesado
- **Puppeteer + puppeteer-extra-stealth** - Anti-detecci√≥n avanzada
- **Cheerio** - Para sites est√°ticos (10x m√°s r√°pido)

#### Queue System
- **BullMQ** - Queue robusto con Redis
- **Upstash Redis** - Redis serverless (perfecto para Vercel)
- Alternativa: **pg-boss** (queue en PostgreSQL)

#### Scheduling
- **Supabase pg_cron** + **Edge Functions** (recomendado)
- Alternativa: **Vercel Cron Jobs**

#### Proxies & Anti-Bot
- **ScraperAPI** - Pay-per-successful-request, maneja CAPTCHA
- Alternativas: Bright Data, Oxylabs (premium)

#### Data Processing
- **Zod** - Validation de datos
- **Geocodio** - Geocoding especializado en real estate
- **ML-based deduplication** con fuzzy matching

---

### Arquitectura Propuesta

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ         Supabase pg_cron (Scheduler)        ‚îÇ
‚îÇ         - Hourly: MercadoLibre API          ‚îÇ
‚îÇ         - Daily: ZonaProp scraping          ‚îÇ
‚îÇ         - Weekly: Properati BigQuery sync   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                 ‚îÇ
                 ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ       Supabase Edge Functions (Triggers)    ‚îÇ
‚îÇ       - Iniciar jobs de scraping            ‚îÇ
‚îÇ       - Enqueue tasks                       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                 ‚îÇ
                 ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ      BullMQ + Upstash Redis (Queue)         ‚îÇ
‚îÇ      Jobs:                                  ‚îÇ
‚îÇ      - scrape_mercadolibre                  ‚îÇ
‚îÇ      - scrape_zonaprop                      ‚îÇ
‚îÇ      - process_raw_listing                  ‚îÇ
‚îÇ      - download_images                      ‚îÇ
‚îÇ      - geocode_address                      ‚îÇ
‚îÇ      - deduplicate                          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                 ‚îÇ
                 ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ     Workers (Vercel Functions o VPS)        ‚îÇ
‚îÇ     - Playwright/Puppeteer instances        ‚îÇ
‚îÇ     - Image processing (Sharp)              ‚îÇ
‚îÇ     - Data normalization                    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                 ‚îÇ
                 ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ         ScraperAPI (Proxies)                ‚îÇ
‚îÇ         - Rotate IPs                        ‚îÇ
‚îÇ         - Solve CAPTCHA                     ‚îÇ
‚îÇ         - Geolocation                       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                 ‚îÇ
                 ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ          Data Pipeline                      ‚îÇ
‚îÇ   RAW ‚Üí Parse ‚Üí Validate ‚Üí Normalize ‚Üí     ‚îÇ
‚îÇ   Enrich ‚Üí Dedupe ‚Üí Store                  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                 ‚îÇ
                 ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ      Supabase PostgreSQL (Storage)          ‚îÇ
‚îÇ      - properties (master)                  ‚îÇ
‚îÇ      - raw_listings                         ‚îÇ
‚îÇ      - price_history                        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

### Configuraci√≥n de Scraping

#### ZonaProp Scraper (Crawlee + Playwright)

```javascript
// zonaprop-scraper.js
import { PlaywrightCrawler, Dataset } from 'crawlee'
import { createClient } from '@supabase/supabase-js'

const crawler = new PlaywrightCrawler({
  // Rate limiting CR√çTICO
  maxConcurrency: 1,
  maxRequestsPerMinute: 20,          // Max 20 req/min = 1 cada 3 seg

  // Stealth
  launchContext: {
    launchOptions: {
      headless: true,
      args: [
        '--disable-blink-features=AutomationControlled',
        '--disable-dev-shm-usage',
        '--no-sandbox'
      ]
    }
  },

  // Request handler
  async requestHandler({ request, page, enqueueLinks, log }) {
    log.info(`Processing: ${request.url}`)

    // Esperar renderizado
    await page.waitForSelector('.listing-card', { timeout: 10000 })

    // Random delay humano
    await page.waitForTimeout(Math.random() * 2000 + 1000)

    // Extraer datos
    const listings = await page.$$eval('.listing-card', cards => {
      return cards.map(card => ({
        url: card.querySelector('a')?.href,
        title: card.querySelector('.title')?.textContent?.trim(),
        price: card.querySelector('.price')?.textContent?.trim(),
        location: card.querySelector('.location')?.textContent?.trim(),
        features: {
          rooms: card.querySelector('.rooms')?.textContent?.trim(),
          surface: card.querySelector('.surface')?.textContent?.trim()
        }
      }))
    })

    // Guardar en dataset
    await Dataset.pushData(listings)

    // Encolar siguiente p√°gina (respetando robots.txt)
    await enqueueLinks({
      selector: '.pagination a.next',
      label: 'NEXT_PAGE'
    })
  },

  // Error handling
  failedRequestHandler({ request, log }) {
    log.error(`Request ${request.url} failed`)
  }
})

// Run
await crawler.run([
  'https://www.zonaprop.com.ar/departamentos-venta-cordoba.html'
])
```

#### MercadoLibre API Client

```javascript
// mercadolibre-client.js
import axios from 'axios'

class MercadoLibreClient {
  constructor(accessToken) {
    this.accessToken = accessToken
    this.baseURL = 'https://api.mercadolibre.com'
  }

  async searchProperties(location, options = {}) {
    const params = {
      category: 'MLA1459',              // Inmuebles
      item_location: location,          // 'lat:-31_-31.5,lon:-64_-64.5'
      operation: options.operation,     // 'sale' | 'rent'
      property_type: options.type,      // 'apartment' | 'house'
      price: options.priceRange,        // '50000-150000'
      limit: options.limit || 50,
      offset: options.offset || 0
    }

    const response = await axios.get(`${this.baseURL}/sites/MLA/search`, {
      params,
      headers: {
        'Authorization': `Bearer ${this.accessToken}`
      }
    })

    return response.data.results
  }

  async getPropertyDetails(itemId) {
    const response = await axios.get(`${this.baseURL}/items/${itemId}`, {
      headers: {
        'Authorization': `Bearer ${this.accessToken}`
      }
    })

    return response.data
  }
}

// Uso
const client = new MercadoLibreClient(process.env.ML_ACCESS_TOKEN)

const cordobaProperties = await client.searchProperties(
  'lat:-31.4_-31.5,lon:-64.1_-64.3',
  {
    operation: 'sale',
    type: 'apartment',
    priceRange: '50000-150000',
    limit: 100
  }
)

for (const property of cordobaProperties) {
  const details = await client.getPropertyDetails(property.id)

  // Guardar en raw_listings
  await supabase.from('raw_listings').insert({
    source_id: mercadolibreSourceId,
    external_id: property.id,
    url: property.permalink,
    raw_data: details,
    title: details.title,
    price_raw: details.price.toString()
  })
}
```

---

### Data Pipeline

```javascript
// data-pipeline.js

class PropertyPipeline {
  constructor(supabase) {
    this.supabase = supabase
  }

  async processRawListing(rawListingId) {
    // 1. Fetch raw listing
    const { data: rawListing } = await this.supabase
      .from('raw_listings')
      .select('*')
      .eq('id', rawListingId)
      .single()

    try {
      // 2. Parse (source-specific)
      const parsed = this.parse(rawListing)

      // 3. Validate
      const validated = this.validate(parsed)

      // 4. Normalize
      const normalized = this.normalize(validated)

      // 5. Enrich
      const enriched = await this.enrich(normalized)

      // 6. Deduplicate
      const duplicates = await this.findDuplicates(enriched)

      if (duplicates.length > 0) {
        // Update existing property
        await this.updateProperty(duplicates[0].id, enriched)
        propertyId = duplicates[0].id
      } else {
        // Create new property
        propertyId = await this.createProperty(enriched)
      }

      // 7. Mark as processed
      await this.supabase
        .from('raw_listings')
        .update({
          processing_status: 'processed',
          processed_at: new Date().toISOString(),
          property_id: propertyId
        })
        .eq('id', rawListingId)

      return propertyId

    } catch (error) {
      // Mark as error
      await this.supabase
        .from('raw_listings')
        .update({
          processing_status: 'error',
          error_message: error.message
        })
        .eq('id', rawListingId)

      throw error
    }
  }

  parse(rawListing) {
    const source = rawListing.source.name
    const rawData = rawListing.raw_data

    switch (source) {
      case 'mercadolibre':
        return this.parseMercadoLibre(rawData)
      case 'zonaprop':
        return this.parseZonaProp(rawData)
      case 'properati':
        return this.parseProperati(rawData)
      default:
        throw new Error(`Unknown source: ${source}`)
    }
  }

  validate(data) {
    // Zod schema validation
    const PropertySchema = z.object({
      property_type: z.enum(['apartment', 'house', 'ph', 'land', 'commercial']),
      operation_type: z.enum(['sale', 'rent', 'temp_rent']),
      price: z.number().positive().optional(),
      currency: z.enum(['ARS', 'USD', 'EUR']).optional(),
      location: z.object({
        lat: z.number().min(-90).max(90).optional(),
        lng: z.number().min(-180).max(180).optional()
      }).optional(),
      // ... more fields
    })

    return PropertySchema.parse(data)
  }

  normalize(data) {
    return {
      ...data,
      // Normalize property type
      property_type: this.normalizePropertyType(data.property_type),

      // Normalize price to USD
      price_usd: data.currency === 'USD'
        ? data.price
        : this.convertToUSD(data.price, data.currency),

      // Normalize address
      address: this.normalizeAddress(data.address),

      // Extract street components
      ...this.parseAddress(data.address)
    }
  }

  async enrich(data) {
    // Geocoding if no coordinates
    if (!data.location?.lat && data.address) {
      const coordinates = await this.geocode(data.address)
      data.location = coordinates
      data.location_confidence = coordinates.confidence
    }

    // Calculate data quality score
    data.data_quality_score = this.calculateQualityScore(data)

    return data
  }

  async findDuplicates(data) {
    if (data.location?.lat) {
      // Search by coordinates (50m radius)
      const { data: matches } = await this.supabase.rpc('find_properties_nearby', {
        p_lat: data.location.lat,
        p_lng: data.location.lng,
        p_radius_m: 50
      })

      return matches.filter(m => m.similarity_score > 0.85)
    }

    // Fallback: fuzzy match by address
    const { data: matches } = await this.supabase.rpc('find_properties_by_address', {
      p_address: data.address,
      p_threshold: 0.8
    })

    return matches
  }

  async createProperty(data) {
    const { data: property } = await this.supabase
      .from('properties')
      .insert({
        ...data,
        location: data.location?.lat
          ? `SRID=4326;POINT(${data.location.lng} ${data.location.lat})`
          : null,
        first_seen_at: new Date().toISOString(),
        last_seen_at: new Date().toISOString(),
        times_seen: 1
      })
      .select()
      .single()

    return property.id
  }

  async updateProperty(propertyId, data) {
    await this.supabase
      .from('properties')
      .update({
        last_seen_at: new Date().toISOString(),
        times_seen: this.supabase.sql`times_seen + 1`,
        // Update fields that changed
        ...(data.price !== undefined && { price: data.price }),
        ...(data.description !== undefined && { description: data.description })
      })
      .eq('id', propertyId)
  }
}
```

---

### Scheduling con Supabase pg_cron

```sql
-- Setup pg_cron extension
CREATE EXTENSION IF NOT EXISTS pg_cron;

-- Job 1: Scrape MercadoLibre every 6 hours
SELECT cron.schedule(
  'scrape-mercadolibre',
  '0 */6 * * *',                    -- Every 6 hours
  $$
  SELECT net.http_post(
    url := 'https://your-edge-function.supabase.co/scrape-mercadolibre',
    headers := '{"Authorization": "Bearer YOUR_SERVICE_KEY"}'::jsonb
  );
  $$
);

-- Job 2: Scrape ZonaProp daily at 3 AM
SELECT cron.schedule(
  'scrape-zonaprop',
  '0 3 * * *',                      -- Daily at 3 AM
  $$
  SELECT net.http_post(
    url := 'https://your-edge-function.supabase.co/scrape-zonaprop',
    headers := '{"Authorization": "Bearer YOUR_SERVICE_KEY"}'::jsonb
  );
  $$
);

-- Job 3: Sync Properati BigQuery weekly
SELECT cron.schedule(
  'sync-properati',
  '0 4 * * 0',                      -- Sundays at 4 AM
  $$
  SELECT net.http_post(
    url := 'https://your-edge-function.supabase.co/sync-properati',
    headers := '{"Authorization": "Bearer YOUR_SERVICE_KEY"}'::jsonb
  );
  $$
);

-- Job 4: Detect inactive properties daily
SELECT cron.schedule(
  'detect-inactive',
  '0 5 * * *',                      -- Daily at 5 AM
  $$SELECT detect_inactive_properties()$$
);

-- Job 5: Deduplication weekly
SELECT cron.schedule(
  'deduplicate',
  '0 6 * * 0',                      -- Sundays at 6 AM
  $$SELECT create_duplicate_clusters()$$
);
```

---

### Queue System con BullMQ

```javascript
// queue-setup.js
import { Queue, Worker } from 'bullmq'
import { Redis } from '@upstash/redis'

// Upstash Redis connection
const connection = {
  host: process.env.UPSTASH_REDIS_HOST,
  port: 6379,
  password: process.env.UPSTASH_REDIS_PASSWORD,
  tls: {}
}

// Define queues
const scrapingQueue = new Queue('scraping', { connection })
const processingQueue = new Queue('processing', { connection })
const imageQueue = new Queue('images', { connection })

// Worker: Scraping
const scrapingWorker = new Worker(
  'scraping',
  async job => {
    const { source, url, options } = job.data

    console.log(`Scraping ${source}: ${url}`)

    // Execute scraping
    const listings = await scrapeSource(source, url, options)

    // Enqueue for processing
    for (const listing of listings) {
      await processingQueue.add('process-listing', {
        source,
        listing
      })
    }

    return { scraped: listings.length }
  },
  {
    connection,
    concurrency: 2,                  // Max 2 concurrent scraping jobs
    limiter: {
      max: 10,                       // Max 10 jobs
      duration: 60000                // Per minute
    }
  }
)

// Worker: Processing
const processingWorker = new Worker(
  'processing',
  async job => {
    const { source, listing } = job.data

    console.log(`Processing listing: ${listing.id}`)

    // Insert raw listing
    const { data: rawListing } = await supabase
      .from('raw_listings')
      .insert({
        source_id: getSourceId(source),
        external_id: listing.id,
        url: listing.url,
        raw_data: listing,
        title: listing.title
      })
      .select()
      .single()

    // Process through pipeline
    const pipeline = new PropertyPipeline(supabase)
    const propertyId = await pipeline.processRawListing(rawListing.id)

    // Enqueue image downloads
    if (listing.images && listing.images.length > 0) {
      await imageQueue.add('download-images', {
        propertyId,
        images: listing.images
      })
    }

    return { propertyId }
  },
  {
    connection,
    concurrency: 10
  }
)

// Worker: Images
const imageWorker = new Worker(
  'images',
  async job => {
    const { propertyId, images } = job.data

    console.log(`Downloading ${images.length} images for property ${propertyId}`)

    for (const [index, imageUrl] of images.entries()) {
      try {
        // Download
        const buffer = await downloadImage(imageUrl)

        // Process (resize, WebP conversion)
        const processed = await processImage(buffer)

        // Upload to Supabase Storage
        const storagePath = `properties/${propertyId}/gallery/${index}.webp`
        await supabase.storage
          .from('properties')
          .upload(storagePath, processed.gallery)

        // Save record
        await supabase.from('property_images').insert({
          property_id: propertyId,
          url: `${storageURL}/${storagePath}`,
          display_order: index
        })

      } catch (error) {
        console.error(`Failed to process image ${imageUrl}:`, error)
      }
    }

    return { processed: images.length }
  },
  {
    connection,
    concurrency: 5
  }
)

// Error handling
scrapingWorker.on('failed', (job, err) => {
  console.error(`Scraping job ${job.id} failed:`, err)
})

processingWorker.on('failed', (job, err) => {
  console.error(`Processing job ${job.id} failed:`, err)
})

// Export queues
export { scrapingQueue, processingQueue, imageQueue }
```

---

### Cumplimiento Legal y √âtico

#### Robots.txt Compliance

```javascript
import { RobotsTxtParser } from 'robots-txt-parser'

class RobotsTxtChecker {
  constructor() {
    this.cache = new Map()
  }

  async isAllowed(url) {
    const domain = new URL(url).hostname

    // Check cache
    if (!this.cache.has(domain)) {
      const robotsTxtUrl = `https://${domain}/robots.txt`
      const parser = new RobotsTxtParser()
      await parser.fetch(robotsTxtUrl)
      this.cache.set(domain, parser)
    }

    const parser = this.cache.get(domain)
    return parser.isAllowed(url, 'RealEstateAggregator')
  }
}

const robotsChecker = new RobotsTxtChecker()

// Antes de cada request
if (!await robotsChecker.isAllowed(targetUrl)) {
  console.log(`Blocked by robots.txt: ${targetUrl}`)
  return
}
```

#### Rate Limiting Respetuoso

```javascript
const rateLimits = {
  'zonaprop.com.ar': {
    requestsPerSecond: 0.33,         // 1 request cada 3 segundos
    requestsPerHour: 1200,           // Max 1200 req/hora
    requestsPerDay: 10000,           // Max 10k req/d√≠a
    activeHours: [2, 3, 4, 5, 6]     // Scraping solo 2-6 AM
  },
  'mercadolibre.com.ar': {
    requestsPerSecond: 2,            // API oficial, m√°s tolerante
    requestsPerHour: 7200
  }
}

class RateLimiter {
  constructor(domain, limits) {
    this.domain = domain
    this.limits = limits
    this.requestLog = []
  }

  async waitIfNeeded() {
    const now = Date.now()
    const currentHour = new Date().getHours()

    // Check active hours
    if (this.limits.activeHours &&
        !this.limits.activeHours.includes(currentHour)) {
      throw new Error(`Scraping not allowed at hour ${currentHour}`)
    }

    // Clean old requests from log
    this.requestLog = this.requestLog.filter(t => now - t < 86400000) // 24h

    // Check daily limit
    if (this.requestLog.length >= this.limits.requestsPerDay) {
      throw new Error('Daily limit reached')
    }

    // Calculate wait time
    const recentRequests = this.requestLog.filter(t => now - t < 1000)
    if (recentRequests.length >= this.limits.requestsPerSecond) {
      const oldestRecent = Math.min(...recentRequests)
      const waitMs = 1000 - (now - oldestRecent)

      if (waitMs > 0) {
        console.log(`Rate limit: waiting ${waitMs}ms`)
        await new Promise(resolve => setTimeout(resolve, waitMs))
      }
    }

    // Log request
    this.requestLog.push(Date.now())
  }
}
```

#### User-Agent Honesto

```javascript
const userAgent = [
  'RealEstateAggregator/1.0',
  '(+https://your-website.com/about-scraping)',
  'contact@your-email.com'
].join(' ')

// En todas las requests
const response = await fetch(url, {
  headers: {
    'User-Agent': userAgent
  }
})
```

---

### Infraestructura y Deployment

#### Opci√≥n A: Serverless (Recomendado para MVP)

**Stack:**
- Vercel Functions (workers)
- Supabase (database + storage + cron)
- Upstash Redis (queue)
- ScraperAPI (proxies)

**Ventajas:**
- Cero mantenimiento
- Auto-scaling
- Pay-per-use
- Setup r√°pido

**Costos MVP:**
- Vercel Pro: $20/mes
- Supabase Pro: $25/mes
- Upstash: $10/mes (25k commands)
- ScraperAPI: $49/mes (50k requests)
- **Total: ~$104/mes + $0.001/scrape**

#### Opci√≥n B: Hybrid (Para escalar)

**Stack:**
- VPS con Docker (workers)
- Supabase (database + storage)
- Redis on VPS (queue)
- ScraperAPI (proxies)

**Ventajas:**
- M√°s control
- Mejor costo a escala
- Performance consistente

**Costos:**
- VPS (8GB RAM): $40/mes
- Supabase Pro: $60/mes
- ScraperAPI: $199/mes (500k requests)
- **Total: ~$299/mes**

#### Docker Compose para Workers

```yaml
# docker-compose.yml
version: '3.8'

services:
  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"
    volumes:
      - redis-data:/data
    command: redis-server --appendonly yes

  scraping-worker:
    build:
      context: .
      dockerfile: Dockerfile.worker
    environment:
      - NODE_ENV=production
      - SUPABASE_URL=${SUPABASE_URL}
      - SUPABASE_KEY=${SUPABASE_KEY}
      - REDIS_URL=redis://redis:6379
      - WORKER_TYPE=scraping
    depends_on:
      - redis
    deploy:
      replicas: 2
      resources:
        limits:
          cpus: '2'
          memory: 2G

  processing-worker:
    build:
      context: .
      dockerfile: Dockerfile.worker
    environment:
      - NODE_ENV=production
      - SUPABASE_URL=${SUPABASE_URL}
      - SUPABASE_KEY=${SUPABASE_KEY}
      - REDIS_URL=redis://redis:6379
      - WORKER_TYPE=processing
    depends_on:
      - redis
    deploy:
      replicas: 4
      resources:
        limits:
          cpus: '1'
          memory: 1G

  image-worker:
    build:
      context: .
      dockerfile: Dockerfile.worker
    environment:
      - NODE_ENV=production
      - SUPABASE_URL=${SUPABASE_URL}
      - SUPABASE_KEY=${SUPABASE_KEY}
      - REDIS_URL=redis://redis:6379
      - WORKER_TYPE=images
    depends_on:
      - redis
    deploy:
      replicas: 2
      resources:
        limits:
          cpus: '1'
          memory: 512M

volumes:
  redis-data:
```

---

## üìã Estrategia de Implementaci√≥n

### Fase 1: Fundaci√≥n Legal (Semanas 1-4)

**Objetivo:** MVP con fuentes 100% legales

#### Tareas
- [ ] Setup Supabase proyecto
- [ ] Implementar schema completo de BD
- [ ] Configurar extensiones (PostGIS, pg_trgm, TimescaleDB)
- [ ] Implementar MercadoLibre OAuth 2.0
  - [ ] Registrar app en Developer Portal
  - [ ] Flujo de autenticaci√≥n
  - [ ] Client para API
- [ ] Configurar acceso a Properati BigQuery
  - [ ] Proyecto Google Cloud
  - [ ] Queries de sincronizaci√≥n
- [ ] Pipeline b√°sico de normalizaci√≥n
- [ ] Setup Supabase Storage para im√°genes
- [ ] Procesamiento b√°sico de im√°genes (Sharp)

#### Entregables
- Base de datos funcional con schema completo
- Ingesta autom√°tica de MercadoLibre
- Sync semanal de Properati
- ~10-15k propiedades en C√≥rdoba

#### M√©tricas de √âxito
- 100% de listings de ML ingestados correctamente
- <5% de errores en normalizaci√≥n
- Deduplicaci√≥n b√°sica funcionando (>80% accuracy)

---

### Fase 2: Expansi√≥n Controlada (Semanas 5-8)

**Objetivo:** Agregar scraping √©tico de ZonaProp

#### Tareas
- [ ] Implementar scraper de ZonaProp
  - [ ] Crawlee + Playwright setup
  - [ ] Rate limiting estricto
  - [ ] Robots.txt compliance
  - [ ] User-agent honesto
- [ ] Sistema de colas con BullMQ + Upstash
- [ ] Deduplicaci√≥n avanzada
  - [ ] Match por coordenadas (PostGIS)
  - [ ] Fuzzy matching de direcciones
  - [ ] Clustering autom√°tico
- [ ] Descarga y procesamiento de im√°genes
  - [ ] Rate limiting por dominio
  - [ ] Multi-tama√±o (thumbnail, card, gallery, full)
  - [ ] Conversi√≥n WebP
  - [ ] BlurHash generation
- [ ] Supabase pg_cron jobs
  - [ ] Scraping nocturno ZonaProp
  - [ ] Detecci√≥n de inactivos
  - [ ] Actualizaci√≥n de precios

#### Entregables
- Scraping automatizado de ZonaProp funcionando
- ~65,000 propiedades totales en C√≥rdoba
- Im√°genes optimizadas y servidas por CDN
- Historial de precios tracking

#### M√©tricas de √âxito
- 0 bloqueos de IP en primera semana
- >90% de im√°genes descargadas exitosamente
- Deduplicaci√≥n >85% accuracy
- <10% de duplicados en sistema

---

### Fase 3: Optimizaci√≥n y Escala (Semanas 9-12)

**Objetivo:** Producci√≥n robusta y escalable

#### Tareas
- [ ] Agregar m√°s fuentes
  - [ ] CordobaProp (si factible)
  - [ ] Inmuebles24
- [ ] Implementar ScraperAPI para anti-bot
- [ ] Geocoding autom√°tico de direcciones sin coordenadas
- [ ] ML-based deduplication
  - [ ] Training dataset de duplicados confirmados
  - [ ] Modelo de similitud
- [ ] API p√∫blica
  - [ ] GraphQL con Hasura o PostgREST
  - [ ] Rate limiting
  - [ ] API keys
- [ ] Monitoreo y observability
  - [ ] Logs estructurados
  - [ ] M√©tricas de scraping
  - [ ] Alertas de errores
- [ ] Frontend b√°sico
  - [ ] B√∫squeda de propiedades
  - [ ] Filtros avanzados
  - [ ] Mapas interactivos
  - [ ] Comparaci√≥n de precios

#### Entregables
- 100,000+ propiedades en sistema
- API p√∫blica documentada
- Frontend funcional
- Monitoreo completo

#### M√©tricas de √âxito
- Uptime >99.5%
- Latencia API <500ms p95
- Cobertura de C√≥rdoba >80% del mercado

---

### Fase 4: Partnerships y Legitimaci√≥n (Semanas 13-16)

**Objetivo:** Modelo de negocio sostenible

#### Tareas
- [ ] Establecer partnerships con inmobiliarias
  - [ ] Feed oficial de datos
  - [ ] Acuerdos de uso de im√°genes
  - [ ] Revenue share o leads
- [ ] Compliance legal completo
  - [ ] Asesor√≠a legal en Argentina
  - [ ] T√©rminos y condiciones
  - [ ] Pol√≠tica de privacidad GDPR/PDPA
  - [ ] DMCA takedown process
- [ ] Features premium
  - [ ] Alertas de precio
  - [ ] An√°lisis de mercado
  - [ ] Valuaci√≥n autom√°tica (AVM)
  - [ ] Recomendaciones personalizadas
- [ ] Monetizaci√≥n
  - [ ] Modelo freemium API
  - [ ] Leads para inmobiliarias
  - [ ] Publicidad contextual

#### Entregables
- Acuerdos con 10+ inmobiliarias
- Compliance legal certificado
- Modelo de monetizaci√≥n activo

---

## ‚öñÔ∏è Consideraciones Legales

### Marco Legal en Argentina

#### Leyes Aplicables

1. **Protecci√≥n de Datos Personales**
   - Ley 25.326 (Argentina Personal Data Protection Act)
   - Vigente desde 2000
   - Regulada por AAIP (Agencia de Acceso a la Informaci√≥n P√∫blica)

2. **Propiedad Intelectual**
   - Ley 11.723 (Derechos de Autor)
   - Copyright de im√°genes y contenido textual
   - Fair use limitado en Argentina

3. **C√≥digo Civil y Comercial**
   - T√©rminos y condiciones contractuales
   - Responsabilidad civil por da√±os

#### An√°lisis por Fuente

| Fuente | Status Legal | Riesgo | Recomendaci√≥n |
|--------|--------------|--------|---------------|
| **MercadoLibre API** | ‚úÖ Legal | Bajo | Usar libremente bajo t√©rminos OAuth |
| **Properati BigQuery** | ‚úÖ Legal | Bajo | Uso permitido para investigaci√≥n |
| **ZonaProp** | ‚ö†Ô∏è Zona Gris | Medio | Scraping √©tico con precauciones |
| **Argenprop** | ‚ùå Prohibido | Alto | Evitar completamente |
| **Inmuebles24** | ‚ö†Ô∏è Zona Gris | Medio | Similar a ZonaProp |
| **La Voz** | ‚ùì Desconocido | Alto | Investigar m√°s antes de implementar |

---

### Estrategia Legal Recomendada

#### 1. Priorizar Fuentes Legales

**Comenzar con:**
- MercadoLibre API (OAuth 2.0)
- Properati BigQuery (dataset p√∫blico)

**Beneficios:**
- 100% legal y documentado
- Sin riesgo de bloqueos
- Soporte oficial
- ~15-20k propiedades C√≥rdoba

#### 2. Scraping √âtico (Si Necesario)

**Solo implementar si:**
- Necesitas mayor cobertura
- Has consultado con abogado
- Puedes asumir riesgo de bloqueo

**Protocolos obligatorios:**
- ‚úÖ Respetar robots.txt al 100%
- ‚úÖ Rate limiting agresivo (1 req/3 seg)
- ‚úÖ User-agent identificable con contacto
- ‚úÖ Horarios nocturnos (2-6 AM)
- ‚úÖ No scraping de datos personales (emails, tel√©fonos)
- ‚úÖ Attribution de fuente en todos los datos

#### 3. Manejo de Im√°genes

**Obligatorio:**
- Documentar source de cada imagen
- Implementar sistema de attribution
- Pol√≠tica de DMCA takedown
- Email de contacto visible

**Mejor pr√°ctica:**
```javascript
// Metadata de imagen
{
  url: "https://cdn.mysite.com/image.webp",
  source_url: "https://zonaprop.com.ar/...",
  source_attribution: "ZonaProp - Inmobiliaria ABC",
  license_type: "source_copyright",
  uploaded_at: "2025-11-10",
  takedown_email: "copyright@mysite.com"
}
```

#### 4. Pol√≠tica de Privacidad GDPR/PDPA

**Aunque Argentina no est√° en GDPR, implementar best practices:**

```markdown
## Pol√≠tica de Privacidad

### Datos que Recopilamos
- Listings de propiedades de fuentes p√∫blicas
- NO recopilamos datos personales directamente de usuarios finales

### Fuentes de Datos
- MercadoLibre (API oficial)
- Properati (dataset p√∫blico)
- [Otras fuentes con attribution]

### Uso de Datos
- Agregaci√≥n y comparaci√≥n de propiedades
- An√°lisis de mercado inmobiliario
- Proveer informaci√≥n a usuarios finales

### Derechos de Propietarios
- Solicitar remoci√≥n de listing
- Actualizar informaci√≥n incorrecta
- Contacto: privacy@mysite.com

### Copyright
- Las im√°genes son propiedad de los portales originales
- Attribution provista en cada listing
- DMCA takedown: copyright@mysite.com
```

#### 5. T√©rminos y Condiciones

**Cl√°usulas cr√≠ticas:**

```markdown
## T√©rminos y Condiciones

### 1. Naturaleza del Servicio
Este servicio agrega informaci√≥n p√∫blicamente disponible de m√∫ltiples
fuentes para facilitar la b√∫squeda de propiedades. NO somos agentes
inmobiliarios ni representantes de las propiedades listadas.

### 2. Fuentes de Datos
Los datos provienen de:
- APIs oficiales (MercadoLibre)
- Datasets p√∫blicos (Properati)
- Web scraping √©tico de fuentes p√∫blicas

Todas las fuentes son debidamente atribuidas.

### 3. Accuracy de Datos
Si bien hacemos nuestro mejor esfuerzo para mantener informaci√≥n
actualizada, NO garantizamos la exactitud, completitud o vigencia
de los datos. Los usuarios deben verificar toda la informaci√≥n
directamente con el vendedor/inmobiliaria.

### 4. Copyright
Las im√°genes y descripciones son propiedad de sus respectivos
due√±os. Si cree que su copyright ha sido violado, contacte
copyright@mysite.com.

### 5. Limitaci√≥n de Responsabilidad
NO somos responsables por decisiones tomadas basadas en nuestra
informaci√≥n. Este servicio es informativo √∫nicamente.

### 6. Ley Aplicable
Estos t√©rminos se rigen por las leyes de la Rep√∫blica Argentina.
Jurisdicci√≥n: [Ciudad/Provincia].
```

---

### Proceso DMCA Takedown

```javascript
// Endpoint para takedown requests
app.post('/api/takedown', async (req, res) => {
  const {
    propertyId,
    imageUrl,
    requestorName,
    requestorEmail,
    reason,
    copyrightProof
  } = req.body

  // Log request
  await supabase.from('takedown_requests').insert({
    property_id: propertyId,
    image_url: imageUrl,
    requestor_name: requestorName,
    requestor_email: requestorEmail,
    reason: reason,
    status: 'pending',
    received_at: new Date().toISOString()
  })

  // Immediately remove image (compliance)
  await supabase
    .from('property_images')
    .update({ status: 'removed', removed_reason: 'dmca_takedown' })
    .eq('url', imageUrl)

  // Send confirmation email
  await sendEmail({
    to: requestorEmail,
    subject: 'DMCA Takedown Request Received',
    body: `Your takedown request for ${imageUrl} has been received
           and processed. The content has been removed.`
  })

  // Notify admin
  await sendEmail({
    to: 'admin@mysite.com',
    subject: 'New DMCA Takedown Request',
    body: `Property: ${propertyId}\nImage: ${imageUrl}\nRequestor: ${requestorName}`
  })

  res.json({
    success: true,
    message: 'Content removed within 24 hours'
  })
})
```

---

### Consulta Legal Recomendada

**Antes de lanzar a producci√≥n:**

1. **Contratar abogado especializado en:**
   - Derecho inform√°tico
   - Propiedad intelectual
   - Derecho comercial

2. **Revisar:**
   - T√©rminos y condiciones
   - Pol√≠tica de privacidad
   - Proceso de DMCA
   - Contratos con inmobiliarias (si aplica)

3. **Jurisdicci√≥n:**
   - Determinar jurisdicci√≥n aplicable
   - Registrar empresa en Argentina si es comercial

**Costos estimados:**
- Consulta inicial: $200-500 USD
- Redacci√≥n de t√©rminos: $500-1,000 USD
- Review anual: $300-500 USD

---

## üí∞ Costos y Presupuesto

### Estimaci√≥n de Costos - MVP (50k propiedades)

#### Infrastructure

| Servicio | Plan | Costo Mensual | Notas |
|----------|------|---------------|-------|
| **Supabase Database** | Pro | $25 | 8GB incluido, luego $0.125/GB |
| **Supabase Storage** | Incluido | +$0 | Dentro de Pro plan |
| **Vercel Hosting** | Pro | $20 | Functions incluidas |
| **Upstash Redis** | Pay-as-you-go | $10 | 25k commands/d√≠a |
| **ScraperAPI** | Hobby | $49 | 50k requests/mes |
| **Google Cloud** (BigQuery) | Pay-as-you-go | $5 | Queries Properati |
| | | **$109/mes** | |

#### Additional Costs (Variable)

| Item | Costo | Notas |
|------|-------|-------|
| **Geocoding** (Geocodio) | $0.50/1k addresses | ~$25/mes para 50k |
| **Domain** | $12/a√±o | .com o .com.ar |
| **SSL Certificate** | $0 | Let's Encrypt (gratis) |
| **Monitoring** (opcional) | $0-29 | Sentry free tier o Basic |
| | **~$27/mes** | |

**Total MVP: ~$136/mes**

---

### Estimaci√≥n de Costos - Growth (500k propiedades)

#### Infrastructure

| Servicio | Plan | Costo Mensual | Notas |
|----------|------|---------------|-------|
| **Supabase Database** | Pro + storage | $60 | 50GB almacenamiento |
| **Supabase Storage** | Pro | +$20 | 100GB im√°genes |
| **Vercel Hosting** | Pro | $20 | Suficiente para tr√°fico |
| **Upstash Redis** | Pay-as-you-go | $50 | 250k commands/d√≠a |
| **ScraperAPI** | Freelancer | $149 | 300k requests/mes |
| **Google Cloud** (BigQuery) | Pay-as-you-go | $10 | |
| | | **$309/mes** | |

#### Additional Costs

| Item | Costo | Notas |
|------|-------|-------|
| **Geocoding** | $250 | 500k addresses |
| **Monitoring** | $29 | Sentry Basic |
| **Backup externo** | $20 | S3 snapshot semanal |
| | **$299/mes** | |

**Total Growth: ~$608/mes**

---

### Estimaci√≥n de Costos - Scale (5M propiedades)

#### Infrastructure (Hybrid VPS)

| Servicio | Plan | Costo Mensual | Notas |
|----------|------|---------------|-------|
| **VPS (Workers)** | 16GB RAM, 4 vCPU | $80 | Hetzner o DigitalOcean |
| **Supabase Database** | Pro + extra | $150 | 500GB almacenamiento |
| **Supabase Storage** | Pro + extra | $100 | 1TB im√°genes |
| **Redis** | VPS self-hosted | $0 | Incluido en VPS |
| **ScraperAPI** | Business | $399 | 2M requests/mes |
| **Google Cloud** (BigQuery) | Pay-as-you-go | $20 | |
| **CDN** (Cloudflare Pro) | Pro | $20 | Mejor performance |
| | | **$769/mes** | |

#### Additional Costs

| Item | Costo | Notas |
|------|-------|-------|
| **Geocoding** | $250 | Batch processing |
| **Monitoring** | $79 | Sentry Team |
| **DevOps/Maintenance** | $500 | Part-time |
| | **$829/mes** | |

**Total Scale: ~$1,598/mes**

---

### Cost Breakdown por Componente

#### 1. Database Storage

**C√°lculo:**
```
Properties: 500k √ó 5 KB = 2.5 GB
Raw Listings: 10M √ó 10 KB = 100 GB
Property History: 50M √ó 3 KB = 150 GB
Price History: 100M √ó 200 B = 20 GB
Images Metadata: 5M √ó 500 B = 2.5 GB

Total: ~275 GB

Supabase Pro: $25 base (8GB) + (275-8) √ó $0.125 = $58/mes
```

#### 2. Image Storage & CDN

**C√°lculo:**
```
50k propiedades √ó 10 im√°genes/propiedad = 500k im√°genes

Sizes:
- Thumbnail (150px): 15 KB
- Card (400px): 50 KB
- Gallery (800px): 150 KB
- Full (1200px): 300 KB
Total por imagen: ~515 KB

Storage: 500k √ó 515 KB = 257 GB

Supabase: $25 base (100GB) + 157GB √ó $0.021 = $28/mes

Bandwidth:
100k views/mes √ó 50 KB (card) = 5 GB
Supabase: $25 incluye 200 GB/mes = $0 adicional
```

#### 3. Scraping & Proxies

**C√°lculo:**
```
ZonaProp: 54k propiedades √ó 2 updates/mes = 108k requests
MercadoLibre: API no cuenta (OAuth gratis)
Properati: BigQuery gratis
Im√°genes: 500k im√°genes √ó 1 download = 500k requests

Total: ~608k requests/mes

ScraperAPI Freelancer: $149 (300k requests)
Necesitas: $149 √ó 2 = $298/mes
```

#### 4. Queue System

**C√°lculo:**
```
Jobs por d√≠a:
- Scraping: 5k propiedades √ó 2 sources = 10k jobs
- Processing: 10k jobs
- Image downloads: 50k jobs

Total: 70k jobs/d√≠a = 2.1M jobs/mes

Upstash Redis:
$10 base (25k commands/d√≠a) + overages
2.1M / 30 / 25k = 2.8x over limit
Estimado: $10 √ó 3 = $30/mes

Alternativa: Redis en VPS = $0
```

---

### ROI y Monetizaci√≥n

#### Modelos de Ingreso Potenciales

**1. API Freemium**
```
Free tier: 100 requests/d√≠a
Basic: $29/mes - 1,000 requests/d√≠a
Pro: $99/mes - 10,000 requests/d√≠a
Enterprise: Custom pricing

Estimado: 50 clientes Basic + 10 Pro = $2,440/mes
```

**2. Leads para Inmobiliarias**
```
Lead price: $5-15 USD por lead calificado
100 leads/mes √ó $10 = $1,000/mes
```

**3. Premium Features**
```
Alertas de precio: $9/mes
An√°lisis de mercado: $19/mes
Valuaci√≥n autom√°tica: $29/mes

Estimado: 100 usuarios premium = $1,500/mes
```

**4. Publicidad Contextual**
```
Google AdSense: $1-3 CPM
10,000 views/d√≠a √ó 30 d√≠as √ó $2 CPM / 1000 = $600/mes
```

**Total Revenue Potential: ~$5,540/mes**

**Break-even:** Con MVP costs de $136/mes, solo necesitas ~25 clientes Basic de API.

---

## üóìÔ∏è Roadmap de Desarrollo

### Timeline Completo (16 semanas)

```
Semanas 1-4: Fundaci√≥n Legal
‚îú‚îÄ Semana 1
‚îÇ  ‚îú‚îÄ Setup Supabase proyecto
‚îÇ  ‚îú‚îÄ Implementar schema de BD
‚îÇ  ‚îî‚îÄ Extensiones PostgreSQL
‚îú‚îÄ Semana 2
‚îÇ  ‚îú‚îÄ MercadoLibre OAuth flow
‚îÇ  ‚îú‚îÄ ML API client
‚îÇ  ‚îî‚îÄ Normalizers por fuente
‚îú‚îÄ Semana 3
‚îÇ  ‚îú‚îÄ Properati BigQuery setup
‚îÇ  ‚îú‚îÄ Sync scripts
‚îÇ  ‚îî‚îÄ Supabase Storage config
‚îî‚îÄ Semana 4
   ‚îú‚îÄ Pipeline de procesamiento
   ‚îú‚îÄ Deduplicaci√≥n b√°sica
   ‚îî‚îÄ Testing end-to-end

Semanas 5-8: Expansi√≥n Controlada
‚îú‚îÄ Semana 5
‚îÇ  ‚îú‚îÄ ZonaProp scraper (Crawlee)
‚îÇ  ‚îú‚îÄ Rate limiting
‚îÇ  ‚îî‚îÄ Robots.txt compliance
‚îú‚îÄ Semana 6
‚îÇ  ‚îú‚îÄ BullMQ + Upstash setup
‚îÇ  ‚îú‚îÄ Queue workers
‚îÇ  ‚îî‚îÄ Supabase pg_cron jobs
‚îú‚îÄ Semana 7
‚îÇ  ‚îú‚îÄ Image downloader
‚îÇ  ‚îú‚îÄ Sharp processing
‚îÇ  ‚îî‚îÄ Multi-size generation
‚îî‚îÄ Semana 8
   ‚îú‚îÄ Deduplicaci√≥n avanzada
   ‚îú‚îÄ PostGIS matching
   ‚îî‚îÄ Performance testing

Semanas 9-12: Optimizaci√≥n y Escala
‚îú‚îÄ Semana 9
‚îÇ  ‚îú‚îÄ M√°s fuentes (Inmuebles24)
‚îÇ  ‚îú‚îÄ ScraperAPI integration
‚îÇ  ‚îî‚îÄ Geocoding autom√°tico
‚îú‚îÄ Semana 10
‚îÇ  ‚îú‚îÄ API p√∫blica (GraphQL)
‚îÇ  ‚îú‚îÄ Rate limiting
‚îÇ  ‚îî‚îÄ Documentaci√≥n
‚îú‚îÄ Semana 11
‚îÇ  ‚îú‚îÄ Monitoreo (Sentry)
‚îÇ  ‚îú‚îÄ Logs estructurados
‚îÇ  ‚îî‚îÄ Alertas
‚îî‚îÄ Semana 12
   ‚îú‚îÄ Frontend b√°sico
   ‚îú‚îÄ B√∫squeda + filtros
   ‚îî‚îÄ Mapas interactivos

Semanas 13-16: Partnerships y Legitimaci√≥n
‚îú‚îÄ Semana 13-14
‚îÇ  ‚îú‚îÄ Outreach a inmobiliarias
‚îÇ  ‚îú‚îÄ Partnerships
‚îÇ  ‚îî‚îÄ Acuerdos de datos
‚îú‚îÄ Semana 15
‚îÇ  ‚îú‚îÄ Asesor√≠a legal
‚îÇ  ‚îú‚îÄ T√©rminos y condiciones
‚îÇ  ‚îî‚îÄ Compliance
‚îî‚îÄ Semana 16
   ‚îú‚îÄ Monetizaci√≥n
   ‚îú‚îÄ Features premium
   ‚îî‚îÄ Launch p√∫blico
```

---

### Milestones y KPIs

#### Milestone 1: MVP Legal (Semana 4)
- ‚úÖ 10,000+ propiedades ingestadas
- ‚úÖ MercadoLibre API funcionando
- ‚úÖ Properati sync autom√°tico
- ‚úÖ Deduplicaci√≥n >80% accuracy
- ‚úÖ 0 errores cr√≠ticos de BD

#### Milestone 2: Full Coverage (Semana 8)
- ‚úÖ 65,000+ propiedades (C√≥rdoba)
- ‚úÖ 5+ fuentes integradas
- ‚úÖ 100,000+ im√°genes procesadas
- ‚úÖ Deduplicaci√≥n >85% accuracy
- ‚úÖ 0 bloqueos de IP

#### Milestone 3: API P√∫blica (Semana 12)
- ‚úÖ API GraphQL documentada
- ‚úÖ 100ms latency p95
- ‚úÖ 10,000 requests/d√≠a capacity
- ‚úÖ 99% uptime
- ‚úÖ Frontend funcional

#### Milestone 4: Revenue (Semana 16)
- ‚úÖ 10+ partnerships activos
- ‚úÖ 25+ API customers
- ‚úÖ $1,000+ MRR
- ‚úÖ Compliance legal completo

---

## üìö Referencias y Recursos

### Documentaci√≥n Oficial

**APIs:**
- MercadoLibre API: https://developers.mercadolibre.com.ar/
- Properati BigQuery: https://www.properati.com.ar/data
- Supabase Docs: https://supabase.com/docs

**Frameworks:**
- Crawlee: https://crawlee.dev/
- Playwright: https://playwright.dev/
- BullMQ: https://docs.bullmq.io/

**PostgreSQL Extensions:**
- PostGIS: https://postgis.net/documentation/
- pg_trgm: https://www.postgresql.org/docs/current/pgtrgm.html
- TimescaleDB: https://docs.timescale.com/

---

### Repositorios GitHub de Referencia

**Scrapers Multi-Portal:**
- https://github.com/mauroeparis/scrappdept (Python)
- https://github.com/rodrigouroz/housing_scrapper (Python)
- https://github.com/Sotrosca/zona-prop-scraper (Python)
- https://github.com/pablol314/scraper-zonaprop (Fork mejorado)

**Crawling Frameworks:**
- https://github.com/apify/crawlee (Node.js)
- https://github.com/scrapy/scrapy (Python)

---

### Herramientas y Servicios

**Scraping:**
- ScraperAPI: https://www.scraperapi.com/
- Bright Data: https://brightdata.com/
- Apify: https://apify.com/

**Geocoding:**
- Geocodio: https://www.geocod.io/
- Nominatim: https://nominatim.org/ (open source)

**Monitoring:**
- Sentry: https://sentry.io/
- Datadog: https://www.datadoghq.com/
- New Relic: https://newrelic.com/

---

### Legal y Compliance

**Leyes Argentina:**
- Ley 25.326 (Protecci√≥n de Datos): http://servicios.infoleg.gob.ar/infolegInternet/anexos/60000-64999/64790/norma.htm
- AAIP: https://www.argentina.gob.ar/aaip

**DMCA:**
- DMCA.com: https://www.dmca.com/
- Copyright Alliance: https://copyrightalliance.org/

---

### Comunidades y Foros

**Stack Overflow:**
- [web-scraping] tag: https://stackoverflow.com/questions/tagged/web-scraping
- [supabase] tag: https://stackoverflow.com/questions/tagged/supabase

**Discord:**
- Supabase Discord: https://discord.supabase.com/
- Crawlee Discord: https://discord.com/invite/jyEM2PRvMU

**Reddit:**
- r/webscraping: https://reddit.com/r/webscraping
- r/Supabase: https://reddit.com/r/Supabase

---

## üéØ Conclusiones y Pr√≥ximos Pasos

### Resumen de Recomendaciones

**1. Stack Tecnol√≥gico:**
- PostgreSQL + Supabase (database)
- Crawlee + Playwright (scraping)
- BullMQ + Upstash (queue)
- Sharp (image processing)
- Supabase Storage + CDN (images)

**2. Estrategia de Datos:**
- Comenzar con fuentes legales (MercadoLibre API + Properati BigQuery)
- Expandir progresivamente a scraping √©tico
- Rate limiting estricto y respetuoso
- Deduplicaci√≥n multi-etapa con PostGIS

**3. Aspectos Legales:**
- Priorizar compliance desde d√≠a 1
- Consultar abogado especializado
- Implementar DMCA takedown
- Buscar partnerships para legitimaci√≥n

**4. Costos:**
- MVP: ~$136/mes
- Growth: ~$608/mes
- Scale: ~$1,598/mes
- ROI positivo desde 25 clientes API

---

### Pr√≥ximos Pasos Inmediatos

**Semana 1:**
1. [ ] Crear cuenta Supabase y proyecto
2. [ ] Registrar app en MercadoLibre Developer Portal
3. [ ] Setup Google Cloud para Properati BigQuery
4. [ ] Inicializar repositorio Git con estructura
5. [ ] Implementar schema de BD (ejecutar DDL completo)

**Semana 2:**
1. [ ] Implementar MercadoLibre OAuth flow
2. [ ] Crear normalizer para datos de ML
3. [ ] Script de sync de Properati
4. [ ] Pipeline b√°sico de procesamiento
5. [ ] Testing de deduplicaci√≥n

**Semana 3:**
1. [ ] Setup Supabase Storage
2. [ ] Implementar procesamiento de im√°genes (Sharp)
3. [ ] Generaci√≥n de multi-tama√±o + BlurHash
4. [ ] Upload a Storage con pol√≠ticas RLS

**Semana 4:**
1. [ ] Testing end-to-end completo
2. [ ] Optimizaci√≥n de queries
3. [ ] Documentaci√≥n t√©cnica
4. [ ] Demo funcional
5. [ ] Planificaci√≥n Fase 2

---

### Checklist de Pre-Lanzamiento

**T√©cnico:**
- [ ] Schema de BD completo e indexado
- [ ] APIs oficiales integradas (ML, Properati)
- [ ] Pipeline de normalizaci√≥n funcionando
- [ ] Deduplicaci√≥n >85% accuracy
- [ ] Im√°genes optimizadas y servidas por CDN
- [ ] Backups autom√°ticos configurados
- [ ] Monitoring y alertas activas

**Legal:**
- [ ] T√©rminos y condiciones redactados
- [ ] Pol√≠tica de privacidad publicada
- [ ] DMCA takedown process implementado
- [ ] Consulta legal completada
- [ ] Attribution de fuentes en UI

**Operacional:**
- [ ] Costos proyectados y aprobados
- [ ] Servidor/infrastructure provisionado
- [ ] Documentaci√≥n t√©cnica completa
- [ ] Runbooks para operaciones comunes
- [ ] On-call rotation definida

---

### Contacto y Soporte

Para implementaci√≥n de este proyecto, recursos adicionales, o consultas:

**Documentos Relacionados:**
- Arquitectura de Base de Datos: Ver secci√≥n completa arriba
- Almacenamiento de Im√°genes: `/Users/agustinmontoya/RECOMENDACION_ALMACENAMIENTO_IMAGENES.md`
- Motor de Scraping: `/Users/agustinmontoya/MOTOR_SCRAPING_INMUEBLES_DISE√ëO_TECNICO.md`

---

**√öltima actualizaci√≥n:** 2025-11-10
**Versi√≥n:** 1.0
**Status:** Ready for Implementation

---

**¬°√âxito con tu proyecto de agregaci√≥n inmobiliaria! üè†üöÄ**
